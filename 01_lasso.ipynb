{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO\n",
    "> Implementaion of the LASSO regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "from sklearn import linear_model \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from fastcore.all import *\n",
    "from babino2020masks.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FirstInChunkSelector(object):\n",
    "    '''Selects first element from each non zero chunk.'''\n",
    "\n",
    "    def __init__(self, clf):\n",
    "        self.clf, self.coef, self.mask = clf, None, None\n",
    "\n",
    "    def select_coef(self):\n",
    "        n_features = len(self.clf.coef_)\n",
    "        no_zero = np.zeros(n_features+1)\n",
    "        no_zero[1:] = self.clf.coef_ != 0\n",
    "        self.mask = np.diff(no_zero)>0\n",
    "        self.mask[0] = True\n",
    "        self.coef = self.clf.coef_[self.mask]\n",
    "        return self.coef\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.select_coef()\n",
    "        return X[:, self.mask]\n",
    "\n",
    "    def get_support(self):\n",
    "        self.select_coef()\n",
    "        return self.mask\n",
    "\n",
    "    def get_number_of_features(self):\n",
    "        self.select_coef()\n",
    "        return sum(self.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LassoICSelector(object):\n",
    "    \"\"\"LASSO regression with FirstInChunkSelector.\"\"\"\n",
    "\n",
    "    def __init__(self, y, criterion, alpha=0.05):\n",
    "        self.lasso = linear_model.LassoLars(alpha=0, max_iter=100000)\n",
    "        self.criterion = criterion\n",
    "        self.selector = FirstInChunkSelector(self.lasso)\n",
    "        self.OLS = sm.OLS\n",
    "        self.X, self.y = self.liniarize_Xy(y)\n",
    "        self.ols = self.OLS(self.y, self.X)\n",
    "        self.ols_results = None\n",
    "        self.final_ols = False\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def liniarize_Xy(self, y):\n",
    "        y = np.log(y)\n",
    "        X = np.tri(len(y))\n",
    "        X = np.cumsum(X, axis=0)[:, 1:]\n",
    "        return X[~np.isnan(y), :], maybe_attr(y[~np.isnan(y)], 'values')\n",
    "        \n",
    "    def transform_to_ols(self, X):\n",
    "        '''Selects only the features of  that X are used by OLS.\n",
    "        Also, adds a coloumn with ones for the intercept.\n",
    "        '''\n",
    "        X_new = self.selector.transform(X)\n",
    "        if self.final_ols: X_new = X[:, self.support]\n",
    "        return np.hstack([X_new, np.ones((X_new.shape[0], 1))])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Selects features and fits the OLS.'''\n",
    "\n",
    "        # select features\n",
    "        X_new = self.transform_to_ols(X)\n",
    "\n",
    "        # fit ols\n",
    "        self.ols = self.OLS(y, X_new)\n",
    "        self.ols_results = self.ols.fit()\n",
    "\n",
    "        # iteratively remove non signicative variables and fit again\n",
    "        mask = self.ols_results.pvalues < self.alpha / len(self.ols_results.pvalues)\n",
    "        mask[0] = True\n",
    "        Xnew = self.transform_to_ols(X)\n",
    "        Xnew = Xnew[:, mask]\n",
    "        self.support = self.selector.get_support()\n",
    "        self.ols = self.OLS(y, Xnew)\n",
    "        self.ols_results = self.ols.fit()\n",
    "        while any(self.ols_results.pvalues[1:] >= self.alpha / len(self.ols_results.pvalues)):\n",
    "            mask[mask] = (self.ols_results.pvalues < self.alpha / len(self.ols_results.pvalues))\n",
    "            mask[0] = True\n",
    "            Xnew = self.transform_to_ols(X)\n",
    "            Xnew = Xnew[:, mask]\n",
    "            self.support = self.selector.get_support()\n",
    "            self.ols = self.OLS(y, Xnew)\n",
    "            self.ols_results = self.ols.fit()\n",
    "\n",
    "        self.support[self.support] = mask[:-1]\n",
    "\n",
    "    def fit_best_alpha(self):\n",
    "        '''Returns the model with the lowest cirterion.'''\n",
    "        X, y = self.X, self.y\n",
    "        self.lasso.fit(X, y)\n",
    "        alphas = self.lasso.alphas_\n",
    "        self.criterions_ = np.zeros(len(alphas))\n",
    "        self.log_liklehods = np.zeros(len(alphas))\n",
    "        \n",
    "        \n",
    "        for i, alpha in enumerate(alphas):\n",
    "            self.lasso.coef_ = self.lasso.coef_path_[:, i]\n",
    "            self.fit(X, y)\n",
    "            self.criterions_[i], self.log_liklehods[i] = self.get_criterion(self.ols.exog, y)\n",
    "        \n",
    "        # we use a list of tuples to find the minimum cirterion value.\n",
    "        # If there are ties, we use the maximum alpha value.\n",
    "        criterions_idx = list(zip(self.criterions_, alphas, range(len(alphas))))\n",
    "        criterion, alpha, idx = min(criterions_idx, key=lambda x: (x[0], -x[1]))\n",
    "        self.lasso.coef_ = self.lasso.coef_path_[:, idx]\n",
    "        self.lasso.alpha = alpha\n",
    "        self.fit(X, y)\n",
    "        self.final_ols = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Predicts y useing the OLS fit.'''\n",
    "        return self.ols.predict(self.ols_results.params, X)\n",
    "\n",
    "    def log_liklihood(self, X, y):\n",
    "        '''Computes the log liklihood assuming normally distributed errors.'''\n",
    "\n",
    "        eps64 = np.finfo('float64').eps\n",
    "\n",
    "        # residuals\n",
    "        R = y - self.predict(X)\n",
    "        sigma2 = np.var(R)\n",
    "\n",
    "        loglike = -0.5 * len(R) * np.log(sigma2)\n",
    "        loglike -= 0.5 * len(R) * np.log(2*np.pi) - 0.5*len(R) + 0.5\n",
    "        return loglike\n",
    "\n",
    "    def get_criterion(self, X, y):\n",
    "        '''Computes AIC or BIC criterion.'''\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        if self.criterion == 'aic':\n",
    "            K = 2  # AIC\n",
    "        elif self.criterion == 'bic':\n",
    "            K = np.log(n_samples)\n",
    "        else:\n",
    "            raise ValueError('criterion should be either bic or aic')\n",
    "\n",
    "        log_like = self.log_liklihood(X, y)\n",
    "        df = X.shape[1]\n",
    "\n",
    "        aic = K * df - 2*log_like\n",
    "        self.criterion_ = aic\n",
    "\n",
    "        return self.criterion_, log_like\n",
    "\n",
    "    def odds_hat_l_u(self):\n",
    "        Xols = self.transform_to_ols(self.X)\n",
    "        yhat = self.ols.predict(self.ols_results.params, Xols)\n",
    "        # from equation 5\n",
    "        odds_hat = np.exp(yhat)\n",
    "        # the error in yhat is\n",
    "        (yhat_std, yhat_l, yhat_u) = wls_prediction_std(self.ols_results, Xols)\n",
    "        oddshat_l = np.exp(yhat-2*yhat_std)\n",
    "        oddshat_u = np.exp(yhat+2*yhat_std)\n",
    "        return odds_hat, oddshat_l, oddshat_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_lasso.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
